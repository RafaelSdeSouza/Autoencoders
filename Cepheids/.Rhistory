mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)
# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)
# generator, from latent space to reconstructed inputs
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)
vae_loss <- function(x, x_decoded_mean){
xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
xent_loss + kl_loss
}
vae %>% compile(optimizer = "rmsprop", loss = vae_loss)
# Data preparation --------------------------------------------------------
Lyrae <- read.table("blg_met_rrl.dat",header=T) %>% mutate(.,Period = log10(Period)) %>%
filter(X.Fe.H. > -1.647874 && X.Fe.H. < -0.279256)
idx <- sample(seq_len(nrow(Lyrae)),replace=F, size = 15000)
x_train <-  as.matrix(Lyrae[idx ,c("Period","X.Fe.H.","R21","Imag")])
x_test <- as.matrix(Lyrae[-idx,c("Period","X.Fe.H.","R21","Imag")])
# Model training ----------------------------------------------------------
vae %>% fit(
x_train, x_train,
shuffle = TRUE,
epochs = epochs,
batch_size = batch_size,
validation_data = list(x_test , x_test )
)
# Visualizations ----------------------------------------------------------
x_train_encoded <- predict(encoder, x_train, batch_size = batch_size)
x_train_encoded  %>%
as_data_frame() %>%
mutate(class = as.factor(iris$Species[idx])) %>%
ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) + geom_point()
x_train
x_test
batch_size <- 5L
original_dim <- 4L
latent_dim <- 1L
intermediate_dim <- 3L
epochs <- 50L
epsilon_std <- 0.1
# Model definition --------------------------------------------------------
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)
# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)
# generator, from latent space to reconstructed inputs
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)
z
x
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
z_log_var
Lyrae$R31
# Parameters --------------------------------------------------------------
batch_size <- 50L
original_dim <- 5L
latent_dim <- 2L
intermediate_dim <- 3L
epochs <- 50L
epsilon_std <- 0.1
# Model definition --------------------------------------------------------
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)
# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)
# generator, from latent space to reconstructed inputs
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)
vae_loss <- function(x, x_decoded_mean){
xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
xent_loss + kl_loss
}
vae %>% compile(optimizer = "rmsprop", loss = vae_loss)
# Data preparation --------------------------------------------------------
Lyrae <- read.table("blg_met_rrl.dat",header=T) %>% mutate(.,Period = log10(Period)) %>%
filter(X.Fe.H. > -1.647874 && X.Fe.H. < -0.279256)
idx <- sample(seq_len(nrow(Lyrae)),replace=F, size = 15000)
x_train <-  as.matrix(Lyrae[idx ,c("Period","X.Fe.H.","R21","R31","Imag")])
x_test <- as.matrix(Lyrae[-idx,c("Period","X.Fe.H.","R21","R31","Imag")])
# Model training ----------------------------------------------------------
vae %>% fit(
x_train, x_train,
shuffle = TRUE,
epochs = epochs,
batch_size = batch_size,
validation_data = list(x_test, x_test )
)
x_train_encoded <- predict(encoder, x_train, batch_size = batch_size)
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) + geom_point()
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) + geom_contour()
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) + stat_contour()
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) +  geom_density2d()
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) +  geom_density2d(fill=..level..)
library(keras)
library(ggplot2)
library(dplyr)
K <- keras::backend()
# Parameters --------------------------------------------------------------
batch_size <- 50L
original_dim <- 5L
latent_dim <- 2L
intermediate_dim <- 3L
epochs <- 50L
epsilon_std <- 1.0
# Model definition --------------------------------------------------------
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)
# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)
# generator, from latent space to reconstructed inputs
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)
vae_loss <- function(x, x_decoded_mean){
xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
xent_loss + kl_loss
}
vae %>% compile(optimizer = "rmsprop", loss = vae_loss)
# Data preparation --------------------------------------------------------
Lyrae <- read.table("blg_met_rrl.dat",header=T) %>% mutate(.,Period = log10(Period)) %>%
filter(X.Fe.H. > -1.647874 && X.Fe.H. < -0.279256)
idx <- sample(seq_len(nrow(Lyrae)),replace=F, size = 15000)
x_train <-  as.matrix(Lyrae[idx ,c("Period","X.Fe.H.","R21","R31","Imag","Vmag")])
x_test <- as.matrix(Lyrae[-idx,c("Period","X.Fe.H.","R21","R31","Imag","Vmag")])
# Model training ----------------------------------------------------------
vae %>% fit(
x_train, x_train,
shuffle = TRUE,
epochs = epochs,
batch_size = batch_size,
validation_data = list(x_test, x_test )
)
# Visualizations ----------------------------------------------------------
x_train_encoded <- predict(encoder, x_train, batch_size = batch_size)
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) +  geom_density2d()
batch_size <- 50L
original_dim <- 6L
latent_dim <- 2L
intermediate_dim <- 3L
epochs <- 50L
epsilon_std <- 1.0
# Model definition --------------------------------------------------------
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)
# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)
# generator, from latent space to reconstructed inputs
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)
vae_loss <- function(x, x_decoded_mean){
xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
xent_loss + kl_loss
}
vae %>% compile(optimizer = "rmsprop", loss = vae_loss)
# Data preparation --------------------------------------------------------
Lyrae <- read.table("blg_met_rrl.dat",header=T) %>% mutate(.,Period = log10(Period)) %>%
filter(X.Fe.H. > -1.647874 && X.Fe.H. < -0.279256)
idx <- sample(seq_len(nrow(Lyrae)),replace=F, size = 15000)
x_train <-  as.matrix(Lyrae[idx ,c("Period","X.Fe.H.","R21","R31","Imag","Vmag")])
x_test <- as.matrix(Lyrae[-idx,c("Period","X.Fe.H.","R21","R31","Imag","Vmag")])
# Model training ----------------------------------------------------------
vae %>% fit(
x_train, x_train,
shuffle = TRUE,
epochs = epochs,
batch_size = batch_size,
validation_data = list(x_test, x_test )
)
# Visualizations ----------------------------------------------------------
x_train_encoded <- predict(encoder, x_train, batch_size = batch_size)
x_train_encoded  %>%
as_data_frame() %>%
ggplot(aes(x = V1, y = V2)) +  geom_density2d()
sampling
layer_lambda(sampling)
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
z
1:(latent_dim)
batch_size <- 50L
original_dim <- 6L
latent_dim <- 1L
intermediate_dim <- 3L
epochs <- 50L
epsilon_std <- 1.0
# Model definition --------------------------------------------------------
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
z
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
library(keras)
library(ggplot2)
library(dplyr)
K <- keras::backend()
# Parameters --------------------------------------------------------------
batch_size <- 50L
original_dim <- 6L
latent_dim <- 1L
intermediate_dim <- 3L
epochs <- 50L
epsilon_std <- 1.0
# Model definition --------------------------------------------------------
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
decoder_h
decoder_mean
h_decoded
x_decoded_mean
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
layer_dense(units = original_dim, activation = "sigmoid")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
h_decoded <- decoder_h(z)
decoder_h(z)
z
batch_size <- 50L
original_dim <- 6L
latent_dim <- 2L
intermediate_dim <- 3L
epochs <- 50L
epsilon_std <- 1.0
# Model definition --------------------------------------------------------
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
z
h_decoded <- decoder_h(z)
decoder_h(z)
library(h2o)
h2o.init()
m_AE_4 = h2o.deeplearning(x = c("x", "y"),
training_frame=cc,
autoencoder=TRUE,
epochs = 500,
score_each_iteration=T,
model_id = "m_AE_4",
train_samples_per_iteration=nrow(cc),
activation = "Tanh",
hidden = c(2,1,2)
)
setwd("~/Documents/GitHub/Autoencoders/Cepheids")
library(h2o)
h2o.init()
m_AE_4 = h2o.deeplearning(x = c("x", "y"),
training_frame=cc,
autoencoder=TRUE,
epochs = 500,
score_each_iteration=T,
model_id = "m_AE_4",
train_samples_per_iteration=nrow(cc),
activation = "Tanh",
hidden = c(2,1,2)
)
Lyrae <- read.table("blg_met_rrl.dat",header=T) %>% mutate(.,Period = log10(Period)) %>%
filter(X.Fe.H. > -1.647874 && X.Fe.H. < -0.279256)
test_index <- sample(seq_len(nrow(Lyrae)),replace=F, size = 20000)
Lyrae_short <- Lyrae[test_index,c("Period","X.Fe.H.","R21","Imag")]
LyraeH20  <- as.h2o(Lyrae)
parts <- h2o.splitFrame(LyraeH20,0.8)
train <- parts[[1]]
test <- parts[[2]]
m_AE_4 = h2o.deeplearning(x = c("Period","X.Fe.H.","R21","R31","Imag","Vmag"),
training_frame=train,
autoencoder=TRUE,
epochs = 500,
score_each_iteration=T,
model_id = "m_AE_4",
train_samples_per_iteration=nrow(train),
activation = "Tanh",
hidden = c(4,2,1,2,4)
)
library(h2o)
h2o.init()
Lyrae <- read.table("blg_met_rrl.dat",header=T) %>% mutate(.,Period = log10(Period)) %>%
filter(X.Fe.H. > -1.647874 && X.Fe.H. < -0.279256)
#test_index <- sample(seq_len(nrow(Lyrae)),replace=F, size = 20000)
#Lyrae_short <- Lyrae[test_index,c("Period","X.Fe.H.","R21","Imag")]
LyraeH20  <- as.h2o(Lyrae)
parts <- h2o.splitFrame(LyraeH20,0.8)
train <- parts[[1]]
test <- parts[[2]]
m_AE_4 = h2o.deeplearning(x = c("Period","X.Fe.H.","R21","R31","Imag","Vmag"),
training_frame=train,
autoencoder=TRUE,
epochs = 500,
score_each_iteration=T,
model_id = "m_AE_4",
train_samples_per_iteration=nrow(train),
activation = "Tanh",
hidden = c(4,2,1,2,4)
)
train
Lyrae
LyraeH20
train
sh = h2o.scoreHistory(m_AE_4)
plot(as.data.frame(h2o.scoreHistory(m_AE_4))$training_mse,type="l")
train_1 <- h2o.deepfeatures(m_AE_4,train,layer=3)
d1 <- as.data.frame(train_1)
dens  <- Mclust(train_1)
summary(dens)
plot(dens, what = "density", data = d1)
train_1
hist(train_1 )
train_1[[1]]
d1 <- as.data.frame(train_1)
hist(d1)
d1
hist(d1$DF.L3.C1)
plot(density(d1$DF.L3.C1))
require(mclust)
dens  <- Mclust(train_1)
summary(dens)
plot(dens, what = "density", data = d1)
plot(as.data.frame(Lyrae_short),col=dens$classification)
plot(as.data.frame(train,col=dens$classification)
plot(as.data.frame(train,col=dens$classification))
plot(as.data.frame(train),col=dens$classification)
train
